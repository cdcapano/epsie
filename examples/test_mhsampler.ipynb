{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of creating and running a Metropolis-Hastings Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will setup a sampler to sample a 2D Gaussian distribution with mean $\\bar{x} = 2, \\bar{y} = 5$ and variance $\\sigma_x^2 = 1$, $\\sigma_y^2 = 2$; $\\sigma^2_{xy} = \\sigma^2_{yx} = 0$, using a prior that is uniform over $x \\in [-20, 20)$, $y\\in[-40, 40)$.\n",
    "\n",
    "We will use Python's multiprocessing to evolve 12 chains using 4 cores. We will demonstrate that resuming a sampler from it's state attribute yields the same results as if we had run continuously. We then make a plot of the posterior. Finally, we make an animation showing how the 12 chains moved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib notebook\n",
    "from matplotlib import pyplot\n",
    "import numpy\n",
    "import randomgen\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import epsie\n",
    "from epsie.samplers import MetropolisHastingsSampler\n",
    "from epsie.proposals import Normal\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model to sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note:*** Below we create a class with several functions to draw samples from the prior and to evaluate the log posterior. This isn't strictly necessary. The only thing the Sampler really requires is a function that it can pass keyword arguments to and get back a tuple of (log likelihood, log prior). However, setting things up as a class will make it convenient to, e.g., draw random samples from the prior for the starting positiions, as well as plot the model later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "class Model(object):\n",
    "    def __init__(self):\n",
    "        # we'll use a 2D Gaussian for the likelihood distribution\n",
    "        self.params = ['x', 'y']\n",
    "        self.mean = [2., 5.]\n",
    "        self.cov = [[1., 0.], [0., 2.]]\n",
    "        self.likelihood_dist = stats.multivariate_normal(mean=self.mean,\n",
    "                                                         cov=self.cov)\n",
    "\n",
    "        # we'll just use a uniform prior\n",
    "        self.prior_bounds = {'x': (-20., 20.),\n",
    "                             'y': (-40., 40.)}\n",
    "        xmin = self.prior_bounds['x'][0]\n",
    "        dx = self.prior_bounds['x'][1] - xmin\n",
    "        ymin = self.prior_bounds['y'][0]\n",
    "        dy = self.prior_bounds['y'][1] - ymin\n",
    "        self.prior_dist = {'x': stats.uniform(xmin, dx),\n",
    "                           'y': stats.uniform(ymin, dy)}\n",
    "\n",
    "    def prior_rvs(self, size=None):\n",
    "        return {p: self.prior_dist[p].rvs(size=size)\n",
    "                for p in self.params}\n",
    "    \n",
    "    def logprior(self, **kwargs):\n",
    "        return sum([self.prior_dist[p].logpdf(kwargs[p]) for p in self.params])\n",
    "    \n",
    "    def loglikelihood(self, **kwargs):\n",
    "        return self.likelihood_dist.logpdf([kwargs[p] for p in self.params])\n",
    "    \n",
    "    def __call__(self, **kwargs):\n",
    "        logp = self.logprior(**kwargs)\n",
    "        if logp == -numpy.inf:\n",
    "            logl = None\n",
    "        else:\n",
    "            logl = self.loglikelihood(**kwargs)\n",
    "        return logl, logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x', 'y']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 13.870382947347743, 'y': -5.973684063761851}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: {'x': 13.870382947347743, 'y': -5.973684063761851}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../epsie/proposals/base.py:97: FutureWarning: Generator is deprecated and will be removed sometime after the release of\n",
      "NumPy 1.21 (or 2 releases after 1.19 if there is a major release).\n",
      "\n",
      "Unique features of Generator have been moved to\n",
      "randomgen.generator.ExtendedGenerator. \n",
      "\n",
      "Now is the time to start using numpy.random.Generator.\n",
      "\n",
      "In the mean time Generator will only be updated for the most egregious bugs.\n",
      "\n",
      "You can silence this warning using \n",
      "\n",
      "import warnings\n",
      "warnings.filterwarnings(\"ignore\", \"Generator\", FutureWarning)\n",
      "\n",
      "  return RandomGenerator(self.bit_generator)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'x': 13.524997905345476, 'y': -6.286451465986629}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "\n",
    "normal = Normal(model.params)\n",
    "\n",
    "p = model.prior_rvs()\n",
    "print('p:', p)\n",
    "\n",
    "normal.jump(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 13.870382947347743, 'y': -5.973684063761851}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and run the sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pool of 4 parallel processes, then initialize the sampler using the model we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nchains = 1\n",
    "nprocs = 4\n",
    "pool = multiprocessing.Pool(nprocs)\n",
    "\n",
    "sampler = MetropolisHastingsSampler(model.params, model, nchains, pool=pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set the starting positions of the chains by drawing random variates from the model's prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.start_position = model.prior_rvs(size=nchains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model.prior_rvs(size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = Normal(model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newp = normal.jump(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal.jump(newp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".jump(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's run it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will evolve each chain in the collection by 256 steps. This is parallelized over the pool of processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.run(256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the history of all of the chains using the `.positions` attribute. This will return a numpy structured array in which the fields are the parameters names (in this case, `'x'` and `'y'`), and with shape `nchains x niterations`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = sampler.positions\n",
    "print('sampler.positions: {}'.format(type(positions)))\n",
    "print('with fields: {}'.format(positions.dtype.names))\n",
    "print('and shape:', positions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This (or any structured array returned by epsie) can be turned into a dictionary of arrays, where the keys are the parameter names, using `epsie.array2dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = epsie.array2dict(sampler.positions)\n",
    "print('sampler.positions: {} with keys/values:'.format(type(positions)))\n",
    "for param in sorted(positions):\n",
    "    print('\"{}\": {} with shape {}'.format(param, type(positions[param]), positions[param].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access the history of log likelihoods and log priors using `sampler.stats`, as well as the acceptance ratios and which jumps were accepted with `sampler.acceptance`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = sampler.stats\n",
    "print('sampler.stats: {}'.format(type(stats)))\n",
    "print('with fields: {}'.format(stats.dtype.names))\n",
    "print('and shape:', stats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptance = sampler.acceptance\n",
    "print('sampler.acceptance: {}'.format(type(acceptance)))\n",
    "print('with fields: {}'.format(acceptance.dtype.names))\n",
    "print('and shape:', acceptance.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model returned \"blobs\" (i.e., the model returns a dictionary along with the logl and logp), then we can also access those using `sampler.blobs`. Similar to `positions`, this would also be a dictionary of arrays with keys given by the names in the dictionary the model returned. However, because our model above returns no blobs, in this case we just get `None`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sampler.blobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual chains can be accessed using the `.chains` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume from a state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampler can be checkpointed by getting its current state with `sampler.state`. To demonstrate this, we'll get the current state of the sampler, then run it for another set of iterations. We'll then create a new sampler, and set it's state to the state we obtained from first sampler. Running the same sampler for the same number of iterations should produce the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the current state\n",
    "state = sampler.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now advance the sampler for another 256 iterations\n",
    "sampler.run(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new sampler, but set it's state to what the original sampler's was after the first 250 iterations\n",
    "sampler2 = MetropolisHastingsSampler(model.params, model, nchains, pool=pool)\n",
    "sampler2.set_state(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now advance the new sampler for 256 iterations\n",
    "# note that we don't have to run set_start first, since the starting positions have been set by set_start\n",
    "sampler2.run(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the current results; they should be the same between sampler2 and sampler\n",
    "print('x:', (sampler.current_positions['x'] == sampler2.current_positions['x']).all())\n",
    "print('y:', (sampler.current_positions['y'] == sampler2.current_positions['y']).all())\n",
    "print('logl:', (sampler.current_stats['logl'] == sampler2.current_stats['logl']).all())\n",
    "print('logp:', (sampler.current_stats['logp'] == sampler2.current_stats['logp']).all())\n",
    "print('acceptance ratio:',\n",
    "      (sampler.acceptance['acceptance_ratio'][:,-1] == sampler2.acceptance['acceptance_ratio'][:,-1]).all())\n",
    "print('accepted:',\n",
    "      (sampler.acceptance['accepted'][:,-1] == sampler2.acceptance['accepted'][:,-1]).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clearing memory and continuing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history of results in memory can be cleared using `.clear()`. Running the sampler after a clear yields the same results as if no clear had been done. This is useful for keeping memory usage down: you can dump results to a file after some number of iterations, clear, then continue.\n",
    "\n",
    "To demonstrate this, we'll clear `sampler2`, then run both `sampler` and `sampler2` for another 512 iterations. We'll then compare the current results; they should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler2.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.run(512)\n",
    "sampler2.run(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the current results; they should be the same between sampler2 and sampler\n",
    "print('x:', (sampler.current_positions['x'] == sampler2.current_positions['x']).all())\n",
    "print('y:', (sampler.current_positions['y'] == sampler2.current_positions['y']).all())\n",
    "print('logl:', (sampler.current_stats['logl'] == sampler2.current_stats['logl']).all())\n",
    "print('logp:', (sampler.current_stats['logp'] == sampler2.current_stats['logp']).all())\n",
    "print('acceptance ratio:',\n",
    "      (sampler.acceptance['acceptance_ratio'][:,-1] == sampler2.acceptance['acceptance_ratio'][:,-1]).all())\n",
    "print('accepted:',\n",
    "      (sampler.acceptance['accepted'][:,-1] == sampler2.acceptance['accepted'][:,-1]).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the posterior\n",
    "Let's create a scatter plot of the posterior. For this, we'll need to throw out some earlier samples for the burn-in period; we'll just assume the first-half of the chains were burn in. We also need to calculate the autocorrelation length of the chains in order to get independent samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_acf(data):\n",
    "    \"\"\"Calculates the autocorrelation of some data\"\"\"\n",
    "    # zero the mean\n",
    "    data = data - data.mean()\n",
    "    # zero-pad to 2 * nearest power of 2\n",
    "    newlen = int(2**(1+numpy.ceil(numpy.log2(len(data)))))\n",
    "    x = numpy.zeros(newlen)\n",
    "    x[:len(data)] = data[:]\n",
    "    # correlate\n",
    "    acf = numpy.correlate(x, x, mode='full')\n",
    "    # drop corrupted region\n",
    "    acf = acf[len(acf)//2:]\n",
    "    # normalize\n",
    "    acf /= acf[0]\n",
    "    return acf\n",
    "\n",
    "def calculate_acl(data):\n",
    "    \"\"\"Calculates the autocorrelation length of some data.\n",
    "    \n",
    "    Algorithm used is from:\n",
    "    N. Madras and A.D. Sokal, J. Stat. Phys. 50, 109 (1988).\n",
    "    \"\"\"\n",
    "    # calculate the acf\n",
    "    acf = calculate_acf(data)\n",
    "    # now the ACL: Following from Sokal, this is estimated\n",
    "    # as the first point where M*tau[k] <= k, where\n",
    "    # tau = 2*cumsum(acf) - 1, and M is a tuneable parameter,\n",
    "    # generally chosen to be = 5 (which we use here)\n",
    "    m = 5\n",
    "    cacf = 2.*numpy.cumsum(acf) - 1.\n",
    "    win = m * cacf <= numpy.arange(len(cacf))\n",
    "    if win.any():\n",
    "        acl = int(numpy.ceil(cacf[numpy.where(win)[0][0]]))\n",
    "    else:\n",
    "        # data is too short to estimate the ACL, just choose\n",
    "        # the length of the data\n",
    "        acl = len(data)\n",
    "    return acl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the chains are completely independent of each other, we can calculate the ACL separately for each chain. However, if you'd like to be more conservative, you can also just take the max over all of the chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the samples; recall that this is a dictionary of \n",
    "# nchains x niterations arrays for each parameter\n",
    "samples = sampler.positions\n",
    "# as we said above, we'll assume the first half\n",
    "# of the chain was burn in\n",
    "burnin_iter = sampler.niterations // 2\n",
    "# set up arrays to store the ACL of each chain and\n",
    "# the thinned chains\n",
    "acls = numpy.zeros(nchains, dtype=int)\n",
    "thinned_arrays = {'x': [], 'y': []}\n",
    "# cycle over the chains, calculating the ACLs and thinning them\n",
    "for ii in range(nchains):\n",
    "    # get the second half of the chains\n",
    "    sx = samples['x'][ii, burnin_iter:]\n",
    "    sy = samples['y'][ii, burnin_iter:]\n",
    "    # compute the acl for each parameter\n",
    "    aclx = calculate_acl(sx)\n",
    "    acly = calculate_acl(sy)\n",
    "    acl = max(aclx, acly)\n",
    "    acls[ii] = acl\n",
    "    # note that we'll thin the arrays starting from the\n",
    "    # end to get the lastest results\n",
    "    thinned_arrays['x'].append(sx[::-1][:acl:][::-1])\n",
    "    thinned_arrays['y'].append(sy[::-1][:acl:][::-1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the ACL of each chain:\n",
    "print(acls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a flattened posterior array\n",
    "posterior = {'x': numpy.concatenate(thinned_arrays['x']),\n",
    "             'y': numpy.concatenate(thinned_arrays['y'])}\n",
    "print(\"Number of independent samples:\", posterior['x'].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram them\n",
    "fig, ax = pyplot.subplots()\n",
    "ax.hist(posterior['x'], bins=10, histtype='step', label='x')\n",
    "ax.hist(posterior['y'], bins=10, histtype='step', label='y')\n",
    "ax.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the mean and variance of our estimated posterior. These should be $\\bar{x} \\approx 2, \\sigma^2_{x} \\approx 1$ and $\\bar{y} \\approx 5, \\sigma^2_{y} \\approx 2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in posterior:\n",
    "    s = posterior[param]\n",
    "    print(param, 'mean: {}'.format(s.mean()), 'var: {}'.format(s.var()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values are close, but not exact. This isn't too surprising since we only have $\\mathcal{O}(100)$ samples. To get more samples, the sampler can be run longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an animation of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the results, we'll create an animation showing how the chains evolved. We'll do this by plotting one point for each chain, with each frame in the animation representing a single iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note: To keep file size down, the animation has not been created for the version of this notebook uploaded to the repository.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare an array to create a density map showing the shape of the model posterior\n",
    "npts = 100\n",
    "xmean, ymean = model.likelihood_dist.mean\n",
    "xsig = model.likelihood_dist.cov[0,0]**0.5\n",
    "ysig = model.likelihood_dist.cov[1,1]**0.5\n",
    "X, Y = numpy.mgrid[xmean-3*xsig:xmean+3*xsig:complex(0, npts),\n",
    "                   ymean-3*ysig:ymean+3*ysig:complex(0, npts)]\n",
    "Z = numpy.zeros(X.shape)\n",
    "for ii in range(Z.shape[0]):\n",
    "    for jj in range(Z.shape[1]):\n",
    "        logl, logp = model(x=X[ii,jj], y=Y[ii,jj])\n",
    "        Z[ii, jj] = numpy.exp(logl+logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll just animate the first 200 iterations; change this to\n",
    "# nframes = xdata.shape[1]\n",
    "# if you want to see all iterations\n",
    "nframes = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pyplot.subplots()\n",
    "\n",
    "positions = sampler.positions\n",
    "xdata = positions['x']\n",
    "ydata = positions['y']\n",
    "\n",
    "# Plot density map showing the shape of the true posterior density\n",
    "ax.imshow(numpy.rot90(Z), extent=[X.min(), X.max(), Y.min(), Y.max()],\n",
    "          aspect='auto', cmap='binary', zorder=-3)\n",
    "\n",
    "# Put an x at the maximum posterior point\n",
    "ax.scatter(model.mean[0], model.mean[1], marker='x', color='w', s=10, zorder=-2)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "# create the scatter points\n",
    "ptsize = 60\n",
    "\n",
    "# we'll include the last bufferlen number of steps a chain visited, having the size and transparency\n",
    "# exponentially damped with each new frame\n",
    "bufferlen = 16\n",
    "alphas = numpy.exp(-4*(numpy.arange(bufferlen))/float(bufferlen))\n",
    "sizes = ptsize * alphas\n",
    "#colors = numpy.array(['C{}'.format(ii) for ii in range(nchains)])\n",
    "colors = numpy.arange(nchains)\n",
    "plts = [ax.scatter(xdata[:, bufferlen-ii-1], ydata[:, bufferlen-ii-1], c=colors, s=sizes[ii],\n",
    "                   edgecolors='w', linewidths=0.5,\n",
    "                   alpha=alphas[ii], zorder=bufferlen-ii, marker='s' if ii==0 else 'o', cmap='jet')\n",
    "        for ii in range(bufferlen)]\n",
    "# put a + showing the average of the chain positions at the current iteration\n",
    "meanplt = ax.scatter(xdata[:,0].mean(), ydata[:,0].mean(), marker='P', c='w', edgecolors='k', linewidths=0.5,\n",
    "                     zorder=bufferlen+1)\n",
    "\n",
    "# add some text giving the iteration\n",
    "itertxt = 'Iteration {}'\n",
    "txt = ax.annotate(itertxt.format(1), (0.03, 0.94), xycoords='axes fraction')\n",
    "\n",
    "def animate(ii):\n",
    "    txt.set_text(itertxt.format(ii+1))\n",
    "    for jj,plt in enumerate(plts):\n",
    "        plt.set_offsets(numpy.array([xdata[:, max(ii-jj, 0)], ydata[:, max(ii-jj, 0)]]).T)\n",
    "    meanplt.set_offsets([xdata[:,ii].mean(), ydata[:,ii].mean()])\n",
    "    # zoom in as it narrows on the result\n",
    "    istart = max(ii-bufferlen, 0)\n",
    "    # smooth it out a bit\n",
    "    xmin = numpy.array([xdata[:, max(istart-kk, 0):].min() for kk in range(50)]).mean()\n",
    "    xmax = numpy.array([xdata[:, max(istart-kk, 0):].max() for kk in range(50)]).mean()\n",
    "    ymin = numpy.array([ydata[:, max(istart-kk, 0):].min() for kk in range(50)]).mean()\n",
    "    ymax = numpy.array([ydata[:, max(istart-kk, 0):].max() for kk in range(50)]).mean()\n",
    "    ax.set_xlim((1.1 if xmin < 1 else 0.9)*xmin, (0.9 if xmax < 1 else 1.1)*xmax)\n",
    "    ax.set_ylim((1.1 if ymin < 1 else 0.9)*ymin, (0.9 if ymax < 1 else 1.1)*ymax)\n",
    "\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames=nframes, interval=160, blit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the animation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani.save('chain_animation.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"chain_animation.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
